# Shortcut-Learning-Revisited-On-the-Linear-Seperability-of-Adversarial-Examples

This paper is currently under review.

Abstract: 

In this study, we present significant findings regarding neural network behavior, particularly focusing on two key aspects: the transferability of adversarial examples and the emergence of "shortcut learning." Firstly, we harness the concept of deep matrix factorization and propose an explicit algorithm to derive matrices that contribute to the weight matrices of linear neural networks. We show that these networks can transfer loss computation from specific input data to seemingly unrelated data, demonstrating perfect correlation or anti-correlation. This sheds light on how deep neural networks generalize, even under simpler linear models. Additionally, we demonstrate how adversarial examples can be generated, showcasing linearly separable adversary noise. Our experiments across distinct datasets illustrate the surprising simplicity of these linear methods in reconciling information between seemingly unrelated samples. 
These findings illuminate "shortcut learning," explaining why neural networks achieve high training accuracy but struggle with inaccurate generalizations, notably demonstrated by their susceptibility to adversarial examples.
